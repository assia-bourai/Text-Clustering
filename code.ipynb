{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4Yl-VWsOjCa"
   },
   "outputs": [],
   "source": [
    "#### python_ncut_lib.py\n",
    "# Copyright (C) 2010 R. Cameron Craddock (cameron.craddock@gmail.com)\n",
    "#\n",
    "# This script is a part of the pyClusterROI python toolbox for the spatially\n",
    "# constrained clustering of fMRI data. It provides the library functions for\n",
    "# performing normalized cut clustering according to:\n",
    "#\n",
    "# Stella Yu and Jianbo Shi, \"Understanding Popout through Repulsion,\" Computer\n",
    "# Vision and Pattern Recognition, December, 2001.  \n",
    "#\n",
    "# Shi, J., & Malik, J. (2000).  Normalized cuts and image segmentation. IEEE\n",
    "# Transactions on Pattern Analysis and Machine Intelligence, 22(8), 888-905.\n",
    "# doi: 10.1109/34.868688.\n",
    "#\n",
    "# Yu, S. X., & Shi, J. (2003). Multiclass spectral clustering. Proceedings Ninth\n",
    "# IEEE International Conference on Computer Vision, (1), 313-319 vol.1. Ieee.\n",
    "# doi: 10.1109/ICCV.2003.1238361.\n",
    "#\n",
    "# This code is a port of the NcutClustering_7 matlab toolbox available here:\n",
    "# http://www.cis.upenn.edu/~jshi/software/\n",
    "#\n",
    "# For more information refer to:\n",
    "#\n",
    "# Craddock, R. C.; James, G. A.; Holtzheimer, P. E.; Hu, X. P. & Mayberg, H. S.\n",
    "# A whole brain fMRI atlas generated via spatially constrained spectral\n",
    "# clustering Human Brain Mapping, 2012, 33, 1914-1928 doi: 10.1002/hbm.21333.\n",
    "#\n",
    "# ARTICLE{Craddock2012,\n",
    "#   author = {Craddock, R C and James, G A and Holtzheimer, P E and Hu, X P and\n",
    "#   Mayberg, H S},\n",
    "#   title = {{A whole brain fMRI atlas generated via spatially constrained\n",
    "#   spectral clustering}},\n",
    "#   journal = {Human Brain Mapping},\n",
    "#   year = {2012},\n",
    "#   volume = {33},\n",
    "#   pages = {1914--1928},\n",
    "#   number = {8},\n",
    "#   address = {Department of Neuroscience, Baylor College of Medicine, Houston,\n",
    "#       TX, United States},\n",
    "#   pmid = {21769991},\n",
    "# } \n",
    "#\n",
    "# Documentation, updated source code and other information can be found at the\n",
    "# NITRC web page: http://www.nitrc.org/projects/cluster_roi/ and on github at\n",
    "# https://github.com/ccraddock/cluster_roi\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "# \n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "####\n",
    "\n",
    "# this scripts requires NumPy (numpy.scipy.org) and SciPy (www.scipy.org) to be\n",
    "# installed in a directory that is accessible through PythonPath \n",
    "import sys\n",
    "from numpy import array, reshape, shape, matrix, ones, zeros, sqrt\n",
    "from numpy import argsort, sign, kron, multiply, divide, abs, asarray\n",
    "from numpy.random import rand\n",
    "from scipy.sparse import csc_matrix, spdiags\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.linalg import norm, svd, LinAlgError\n",
    "import numpy as np\n",
    "\n",
    "# exception hander for singular value decomposition\n",
    "class SVDError(Exception):\n",
    "    def __init__(self,value):\n",
    "        self.value=value\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "# (eigen_val, eigen_vec) = ncut( W, nbEigenValues ):\n",
    "#\n",
    "# This function performs the first step of normalized cut spectral clustering.\n",
    "# The normalized LaPlacian is calculated on the similarity matrix W, and top\n",
    "# nbEigenValues eigenvectors are calculated. The number of eigenvectors\n",
    "# corresponds to the maximum number of classes (K) that will be produced by the\n",
    "# clustering algorithm. \n",
    "#\n",
    "#    W:             symmetric #feature x #feature sparse matrix representing the\n",
    "#                   similarity between voxels, traditionally this matrix should\n",
    "#                   be positive semidefinite, but regularization is employed to\n",
    "#                   allow negative matrix entries (Yu 2001)\n",
    "#    nvEigenValues: number of eigenvectors that should be calculated, this\n",
    "#                   determines the maximum number of clusters (K) that can be\n",
    "#                   derived from the\n",
    "#    result\n",
    "#    eigen_val:     (output) eigenvalues from the eigen decomposition of the\n",
    "#                   LaPlacian of W\n",
    "#    eigen_vec:     (output) eigenvectors from the eign decomposition of the\n",
    "#                   LaPlacian of W\n",
    "#\n",
    "def ncut( W, nbEigenValues ):\n",
    "    # parameters\n",
    "    offset=.5\n",
    "    maxiterations=100\n",
    "    eigsErrorTolerence=1e-6\n",
    "    eps=2.2204e-16\n",
    "\n",
    "    m=shape(W)[1]\n",
    "\n",
    "    # make sure that W is symmetric, this is a computationally expensive\n",
    "    # operation, only use for debugging\n",
    "    # if (W-W.transpose()).sum() != 0:\n",
    "    #    print \"W should be symmetric!\"\n",
    "    #    exit(0)\n",
    "\n",
    "    # degrees and regularization\n",
    "    # S Yu Understanding Popout through Repulsion CVPR 2001\n",
    "    # Allows negative values as well as improves invertability of d for small\n",
    "    # numbers i bet that this is what improves the stability of the eigen\n",
    "    d=abs(W).sum(0)\n",
    "    dr=0.5*(d-W.sum(0))\n",
    "    d=d+offset*2\n",
    "    dr=dr+offset\n",
    "\n",
    "    # calculation of the normalized LaPlacian\n",
    "    W=W+spdiags(dr,[0],m,m,\"csc\")\n",
    "    Dinvsqrt=spdiags((1.0/sqrt(d+eps)),[0],m,m,\"csc\")\n",
    "    P=Dinvsqrt*(W*Dinvsqrt);\n",
    "\n",
    "    # perform the eigen decomposition\n",
    "    eigen_val,eigen_vec=eigsh(P,nbEigenValues,maxiter=maxiterations,\\\n",
    "        tol=eigsErrorTolerence,which='LA')\n",
    "\n",
    "    # sort the eigen_vals so that the first\n",
    "    # is the largest\n",
    "    i=argsort(-eigen_val)\n",
    "    eigen_val=eigen_val[i]\n",
    "    eigen_vec=eigen_vec[:,i]\n",
    "\n",
    "    # normalize the returned eigenvectors\n",
    "    eigen_vec=Dinvsqrt*matrix(eigen_vec)\n",
    "    norm_ones=norm(ones((m,1)))\n",
    "    for i in range(0,shape(eigen_vec)[1]):\n",
    "        eigen_vec[:,i]=(eigen_vec[:,i] / norm(eigen_vec[:,i]))*norm_ones\n",
    "        if eigen_vec[0,i] != 0:\n",
    "            eigen_vec[:,i] = -1 * eigen_vec[:,i] * sign( eigen_vec[0,i] )\n",
    "\n",
    "    return(eigen_val, eigen_vec)\n",
    "\n",
    "# eigenvec_discrete=discretisation( eigen_vec ):\n",
    "#\n",
    "# This function performs the second step of normalized cut clustering which\n",
    "# assigns features to clusters based on the eigen vectors from the LaPlacian of\n",
    "# a similarity matrix. There are a few different ways to perform this task. Shi\n",
    "# and Malik (2000) iteratively bisect the features based on the positive and\n",
    "# negative loadings of the eigenvectors. Ng, Jordan and Weiss (2001) proposed to\n",
    "# perform K-means clustering on the rows of the eigenvectors. The method\n",
    "# implemented here was proposed by Yu and Shi (2003) and it finds a discrete\n",
    "# solution by iteratively rotating a binaised set of vectors until they are\n",
    "# maximally similar to the the eigenvectors (for more information, the full\n",
    "# citation is at the top of this file). An advantage of this method over K-means\n",
    "# is that it is _more_ deterministic, i.e. you should get very similar results\n",
    "# every time you run the algorithm on the same data.\n",
    "#\n",
    "# The number of clusters that the features are clustered into is determined by\n",
    "# the number of eignevectors (number of columns) in the input array eigen_vec. A\n",
    "# caveat of this method, is that number of resulting clusters is bound by the\n",
    "# number of eignevectors, but it may contain less.\n",
    "#\n",
    "#    eigen_vec:          Eigenvectors of the normalized LaPlacian calculated\n",
    "#                        from the similarity matrix for the corresponding\n",
    "#                        clustering problem\n",
    "#    eigen_vec_discrete: (output) discretised eigenvectors, i.e. vectors of 0\n",
    "#                        and 1 which indicate whether or not a feature belongs\n",
    "#                        to the cluster defined by the eigen vector.  I.E. a one\n",
    "#                        in the 10th row of the 4th eigenvector (column) means\n",
    "#                        that feature 10 belongs to cluster #4.\n",
    "# \n",
    "def discretisation( eigen_vec ):\n",
    "    eps=2.2204e-16\n",
    "\n",
    "    # normalize the eigenvectors\n",
    "    [n,k]=shape(eigen_vec)\n",
    "    vm=kron(ones((1,k)),sqrt(multiply(eigen_vec,eigen_vec).sum(1)))\n",
    "    eigen_vec=divide(eigen_vec,vm)\n",
    "\n",
    "    svd_restarts=0\n",
    "    exitLoop=0\n",
    "\n",
    "    ### if there is an exception we try to randomize and rerun SVD again\n",
    "        ### do this 30 times\n",
    "    while (svd_restarts < 30) and (exitLoop==0):\n",
    "\n",
    "        # initialize algorithm with a random ordering of eigenvectors\n",
    "        c=zeros((n,1))\n",
    "        R=matrix(zeros((k,k)))\n",
    "        R[:,0]=eigen_vec[int(rand(1)*(n-1)),:].transpose()\n",
    "\n",
    "        for j in range(1,k):\n",
    "            c=c+abs(eigen_vec*R[:,j-1])\n",
    "            R[:,j]=eigen_vec[c.argmin(),:].transpose()\n",
    "\n",
    "\n",
    "        lastObjectiveValue=0\n",
    "        nbIterationsDiscretisation=0\n",
    "        nbIterationsDiscretisationMax=20\n",
    "\n",
    "        # iteratively rotate the discretised eigenvectors until they\n",
    "        # are maximally similar to the input eignevectors, this \n",
    "        # converges when the differences between the current solution\n",
    "        # and the previous solution differs by less than eps or we\n",
    "        # we have reached the maximum number of itarations\n",
    "        while exitLoop == 0:\n",
    "            nbIterationsDiscretisation = nbIterationsDiscretisation + 1\n",
    "\n",
    "            # rotate the original eigen_vectors\n",
    "            tDiscrete=eigen_vec*R\n",
    "\n",
    "            # discretise the result by setting the max of each row=1 and\n",
    "            # other values to 0\n",
    "            j=reshape(asarray(tDiscrete.argmax(1)),n)\n",
    "            eigenvec_discrete=csc_matrix((ones(len(j)),(range(0,n), \\\n",
    "                array(j))),shape=(n,k))\n",
    "\n",
    "            # calculate a rotation to bring the discrete eigenvectors cluster to\n",
    "            # the original eigenvectors\n",
    "            tSVD=eigenvec_discrete.transpose()*eigen_vec\n",
    "            # catch a SVD convergence error and restart\n",
    "            try:\n",
    "                U, S, Vh = svd(tSVD)\n",
    "            except LinAlgError:\n",
    "                # catch exception and go back to the beginning of the loop\n",
    "                print >> sys.stderr, \\\n",
    "                    \"SVD did not converge, randomizing and trying again\"\n",
    "                break\n",
    "\n",
    "            # test for convergence\n",
    "            NcutValue=2*(n-S.sum())\n",
    "            if((abs(NcutValue-lastObjectiveValue) < eps ) or \\\n",
    "                      ( nbIterationsDiscretisation > \\\n",
    "                        nbIterationsDiscretisationMax )):\n",
    "                exitLoop=1\n",
    "            else:\n",
    "                # otherwise calculate rotation and continue\n",
    "                lastObjectiveValue=NcutValue\n",
    "                R=matrix(Vh).transpose()*matrix(U).transpose()\n",
    "\n",
    "    if exitLoop == 0:\n",
    "        raise SVDError(\"SVD did not converge after 30 retries\")\n",
    "    else:\n",
    "        return(eigenvec_discrete)\n",
    "\n",
    "    \n",
    "class ClusteringNormalizedCuts:\n",
    "    \n",
    "    def __init__(self, n_clusters=4):\n",
    "        self.__n_clusters = n_clusters\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.__eigen_val, self.__eigen_vec = ncut(X, self.__n_clusters)\n",
    "        res = discretisation(self.__eigen_vec).toarray()\n",
    "        self.labels_ = np.zeros(X.shape[0])\n",
    "        for i in range(res.shape[1]):\n",
    "            self.labels_[res[:, i] == 1] = i\n",
    "            \n",
    "            \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WVPRNYtPWBP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from  scipy.spatial.distance import cdist\n",
    "from functools import partial\n",
    "import multiprocessing as mlp\n",
    "\n",
    "class KMeans:\n",
    "    \"\"\"\n",
    "        K-Means clustering.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_cluters : int, default=4\n",
    "            The number of clusters to form as well as the number of centroids to generate.\n",
    "            \n",
    "        init : {'k-means++', 'random'}, default='k-means++'\n",
    "            Method for initialization, defaults to 'k-means++':\n",
    "            \n",
    "            'k-means++' : selects initial cluster centers for k-mean \n",
    "            clustering in a smart way to speed up convergence.\n",
    "            \n",
    "            'random': choose k observations (rows) at random from data for\n",
    "            the initial centroids.\n",
    " \n",
    "        max_iter : int, default=100\n",
    "            Maximum number of iterations of the k-means algorithm for a single run.\n",
    "                 \n",
    "        dist : str, default='euclidean'\n",
    "            The distance metric to use.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
    "            Coordiantes of cluster center.\n",
    "        \n",
    "        labels_ : ndarray of shape (n_samples, )\n",
    "            Labels of each point.\n",
    "        \n",
    "        inertia_ : float\n",
    "            Sum of squared distances of samples to their closest cluster center.\n",
    "            \n",
    "        n_iter_ : int\n",
    "            Number of iterations run.   \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_cluters=4, init='k-means++', max_iter=100, dist=\"euclidean\"):\n",
    "        \"\"\"\n",
    "            Initialize self.\n",
    "        \"\"\"\n",
    "        self.__n_cluters = n_cluters\n",
    "        self.__init = init\n",
    "        self.__max_iter = max_iter\n",
    "        self.__dist = dist\n",
    "\n",
    "    def __initialisation(self, X):\n",
    "        if self.__init == 'k-means++':\n",
    "            self.cluster_centers_ = X[np.random.randint(0, X.shape[0], 1), :]\n",
    "            for i in range(self.__n_cluters-1):\n",
    "                dist = np.min(cdist(X, self.cluster_centers_, self.__dist), axis=1)\n",
    "                self.cluster_centers_ = np.concatenate([self.cluster_centers_, \n",
    "                                                       X[np.argmax(dist), :].reshape(1,X.shape[1])], \n",
    "                                                       axis=0)\n",
    "        else:\n",
    "            self.cluster_centers = X[np.random.randint(0, X.shape[0], 3), :]\n",
    "\n",
    "    def __affecte_cluster(self, X):\n",
    "        dist = cdist(X, self.cluster_centers_, self.__dist)\n",
    "        return np.argmin(dist, axis=1), np.min(dist, axis=1)\n",
    "\n",
    "    def __nouveaux_centroides(self, X):\n",
    "        return np.array([list((X[self.labels_ == i]).mean(axis=0)) for i in np.unique(self.labels_)])\n",
    "\n",
    "    def __inertie_globale(self, cluster_dist):\n",
    "        return np.sum([np.power(cluster_dist[self.labels_ == i], 2).sum() for i in np.unique(self.labels_)])\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "            Compute k-means clustering.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            X : array, shape=(n_samples, n_features)\n",
    "                Training instances to cluster.\n",
    "            \n",
    "            Returns \n",
    "            -------\n",
    "            self\n",
    "                Fitted estimator.\n",
    "        \"\"\"\n",
    "        self.__initialisation(X)\n",
    "        self.inertia_ = float('inf')\n",
    "        self.n_iter_ = 0\n",
    "        for i in range(self.__max_iter):\n",
    "            self.n_iter_ += 1\n",
    "            if self.n_iter_ != 1:\n",
    "                self.cluster_centers_ = self.__nouveaux_centroides(X)\n",
    "            self.labels_, cluster_dist = self.__affecte_cluster(X)\n",
    "            new_inertia = self.__inertie_globale(cluster_dist)\n",
    "            if new_inertia == self.inertia_:\n",
    "                self.inertia_ = new_inertia\n",
    "                break\n",
    "                \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "            Compute cluster centers and predict cluster index for each sample.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            X : array, shape=(n_samples, n_features)\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            labels : array, shape [n_samples,]\n",
    "                Index of the cluster each sample belongs to.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "\n",
    "    \n",
    "    \n",
    "def average_confidence(CM, labels):\n",
    "    \"\"\"\n",
    "        Compute the Average Confidence of assignment of the objects to its clusters.\n",
    "         \n",
    "        Parameters\n",
    "        ----------\n",
    "        CM : ndarray of shape(n_samples, n_samples)\n",
    "            Co-association matrix.\n",
    "         \n",
    "        labels : ndarray of shape(n_samples, )\n",
    "            Labels of clustering.\n",
    "         \n",
    "        Return\n",
    "        ------\n",
    "        float\n",
    "            Average confidence of assignment of the objects to its clusters.\n",
    "    \"\"\"\n",
    "    # compute the degree of confidence of assigning an object xi to its cluster Cp.\n",
    "    n_cluster = np.unique(labels).size\n",
    "    conf = np.zeros(labels.size)\n",
    "    for xi in range(labels.size):\n",
    "        xi_lab = labels[xi]\n",
    "        smc = CM[xi, (labels == xi_lab) & (np.arange(labels.size) != xi)]\n",
    "        if smc.size == 0 :\n",
    "            smc = 0\n",
    "        else: \n",
    "            smc = smc.mean()\n",
    "        smac = []\n",
    "        for j in range(n_cluster):\n",
    "            if j != labels[xi]:\n",
    "                smac.append(CM[xi, labels == j].mean())\n",
    "        if len(smac) == 0:\n",
    "            conf[xi] = smc\n",
    "        else:\n",
    "            conf[xi] = (smc - np.max(smac))\n",
    "    return conf.mean()\n",
    "\n",
    "\n",
    "def average_neighborhood_confidence(CM, labels, m=3):\n",
    "    \"\"\"\n",
    "        Compute the Average Neighborhood Confidence of assigning the objects to its clusters.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        CM : ndarray of shape(n_samples, n_samples)\n",
    "            Co-association matrix.\n",
    "         \n",
    "        labels : ndarray of shape(n_samples, )\n",
    "            Labels of clustering.\n",
    "            \n",
    "        m : int, default=3\n",
    "            Number of neighbors.\n",
    "         \n",
    "        Return\n",
    "        ------\n",
    "         float\n",
    "             Average  neighborhood confidence of assignment of the objects to its clusters.   \n",
    "    \"\"\"\n",
    "    n_cluster = np.unique(labels).size\n",
    "    conf = np.zeros(labels.size)\n",
    "    if m == 0:\n",
    "        return conf \n",
    "    for xi in range(labels.size):\n",
    "        xi_lab = labels[xi]\n",
    "        smc = CM[xi, (labels == xi_lab) & (np.arange(labels.size) != xi)]\n",
    "        if smc.size == 0 :\n",
    "            smc = 0\n",
    "        elif smc.size >= m:\n",
    "            smc = np.sort(smc)[::-1][:m].mean()\n",
    "        else:\n",
    "            smc = smc.mean()\n",
    "        smac = []\n",
    "        for j in range(n_cluster):\n",
    "            if j != labels[xi]:\n",
    "                smac_ = CM[xi, labels == j]\n",
    "                if smac_.size > 0:\n",
    "                    if smac_.size >= m:\n",
    "                        smac_ = np.sort(smac_)[::-1][:m]\n",
    "                    smac.append(smac_.mean())\n",
    "        if len(smac) == 0:\n",
    "            conf[xi] = smc\n",
    "        else:\n",
    "            conf[xi] = (smc - np.max(smac))\n",
    "    return conf.mean()\n",
    "\n",
    "\n",
    "def average_dynamique_neighborhood_confidence(CM, labels, alpha=0.5):\n",
    "    \"\"\"\n",
    "        Compute the Average Dynamic Neighborhood Confidence of assigning the objects to its clusters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        CM : ndarray of shape(n_samples, n_samples)\n",
    "            Co-association matrix.\n",
    "         \n",
    "        labels : ndarray of shape(n_samples, )\n",
    "            Labels of clustering.\n",
    "            \n",
    "        alpha : float, default=0.5\n",
    "            Use to calculate the number of neighbors dynamically.\n",
    "         \n",
    "        Return\n",
    "        ------\n",
    "         float\n",
    "             Average dynamic neighborhood confidence of assigning the objects to its clusters.\n",
    "    \"\"\"\n",
    "    n_cluster = np.unique(labels).size\n",
    "    conf = np.zeros(labels.size)\n",
    "    m = 1\n",
    "    for xi in range(labels.size):\n",
    "        xi_lab = labels[xi]\n",
    "        smc = CM[xi, (labels == xi_lab) & (np.arange(labels.size) != xi)]\n",
    "        m = int(np.floor(alpha * smc.sum()))\n",
    "        if m == 0:\n",
    "            m = 1\n",
    "        if smc.size == 0 :\n",
    "            smc = 0\n",
    "        elif smc.size >= m:\n",
    "            smc = np.sort(smc)[::-1][:m].mean()\n",
    "        else:\n",
    "            smc = smc.mean()\n",
    "        smac = []\n",
    "        for j in range(n_cluster):\n",
    "            if j != labels[xi]:\n",
    "                smac_ = CM[xi, labels == j]\n",
    "                if smac_.size > 0:\n",
    "                    if smac_.size >= m:\n",
    "                        smac_ = np.sort(smac_)[::-1][:m]\n",
    "                    smac.append(smac_.mean())\n",
    "        if len(smac) == 0:\n",
    "            conf[xi] = smc\n",
    "        else:\n",
    "            conf[xi] = (smc - np.max(smac))\n",
    "    return conf.mean()\n",
    "\n",
    "\n",
    "def vat(CM):\n",
    "    \"\"\"\n",
    "        Compute the matrix for Visual Assessment of cluster Tendency.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        CM : ndarray of shape(n_samlpes, n_samples)\n",
    "            Co-association matrix, dissimilarity data.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        CM_VAT : ndarray of shape(n_samples, n_samples)\n",
    "            Reorder dissimilarity data.\n",
    "        \n",
    "        Q : ndarray of shape (n_samples, )\n",
    "            Reorder indexs of CM.\n",
    "    \"\"\"\n",
    "    J = np.zeros((CM.shape), dtype=bool)\n",
    "    I = np.ones((CM.shape), dtype=bool)\n",
    "    Q = []\n",
    "    cm_mask = None\n",
    "    for t in range(CM.shape[0]):\n",
    "        if t == 0:\n",
    "            cm_mask = np.ma.masked_array(CM, mask = (I&J))\n",
    "        else:\n",
    "            cm_mask = np.ma.masked_array(CM, mask = (I|J))    \n",
    "        i, j = np.unravel_index(cm_mask.argmin(), CM.shape)\n",
    "        Q.append(j)\n",
    "        J[:, j] = True\n",
    "        I[j, :] = False\n",
    "    cm_vat = CM.copy()\n",
    "    for i in range(CM.shape[0]):\n",
    "        for j in range(CM.shape[0]):\n",
    "            cm_vat[i, j] = CM[Q[i], Q[j]]\n",
    "            cm_vat[j, i] = CM[Q[i], Q[j]]\n",
    "    return cm_vat, np.array(Q)\n",
    "\n",
    "\n",
    "def gen_base_partition(X, k, It):\n",
    "    return KMeans(k, max_iter=It).fit_predict(X)\n",
    "\n",
    "\n",
    "def gen_base_partition_by_kmeans(X, M=10, It=4, Ktype='Random'):\n",
    "    \"\"\"\n",
    "        Generate base partition by k-means\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            data set.\n",
    "        \n",
    "        M : int default=10\n",
    "            The number of base partitions\n",
    "        \n",
    "        It : int, default=10\n",
    "            The number of iterations for k-means\n",
    "        \n",
    "        Ktype : {'Fixed', 'Random'}, default=Random\n",
    "            The type to generate base partitions, 'Fixed' : k = sqrt(N)\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        PI : ndarray of shape(n_sample, n_partitions)\n",
    "            Base partitions, one column is a partition\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "        \n",
    "    PI = []\n",
    "    CM =  CM = np.zeros((X.shape[0], X.shape[0]))\n",
    "    K = 0\n",
    "    if Ktype == 'Fixed':\n",
    "        k = [int(np.ceil(np.sqrt(N)))]*M\n",
    "    else:\n",
    "        k = np.random.randint(2, np.ceil(np.sqrt(N)), M)\n",
    "    kmeans = partial(gen_base_partition, X, It=It)\n",
    "    \n",
    "    pool = mlp.Pool(mlp.cpu_count())\n",
    "    \n",
    "    PI = pool.map(kmeans, k)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.transpose(PI)\n",
    "\n",
    "\n",
    "\n",
    "def gen_cm(PI):\n",
    "    \"\"\"\n",
    "        Generate Co-association Matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        PI : ndarray of shape(n_samples, n_partitions)\n",
    "            Base partitions, one column is a partition.\n",
    "            \n",
    "        Return\n",
    "        ------\n",
    "        CM : ndarray of shape(n_samples, n_samples)\n",
    "            Co-association matrice.\n",
    "    \"\"\"\n",
    "    n_samples = PI.shape[0]\n",
    "    n_partitions = PI.shape[1]\n",
    "    cm = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        cm[i, :] = (PI == PI[i, :]).sum(axis=1)\n",
    "    return cm/n_partitions\n",
    "\n",
    "class CluterEnsemble:\n",
    "    \n",
    "    def __init__(self, n_clusters=4, n_partitions=1000, max_iter=4, k_type='Fixed', cons_validation='ac', m=3, alpha=0.5):\n",
    "        \"\"\"\n",
    "            Clustering Ensemble.\n",
    "            \n",
    "            Operating principle:\n",
    "            ---------\n",
    "                1 - generalization of several partitions using the K Means algorithm with k \n",
    "                    varies between 2 and sqrt (n_sample).\n",
    "\n",
    "                2 - update of the co-association matrix.\n",
    "\n",
    "                3 - generation of partitions by applying the clustering algorithm based on normalized cuts \n",
    "                    on the co_assiation matrix when the negative proof is removed.\n",
    "\n",
    "                4 - selection of the final score with a higher degree of confidence.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            n_cluters : int, default=4\n",
    "                The number of clusters.\n",
    "\n",
    "            n_partitions : int, default=1000\n",
    "                Number of base partitions.\n",
    "\n",
    "            max_iter : int, default=100\n",
    "                Maximum number of iterations of the k-means algorithm for a single run.\n",
    "\n",
    "            k_type : {'Fixed', 'Random'}, default='Fixed'\n",
    "                The type to generate base partitions, \n",
    "            'Fixed' : k = sqrt(n_sample),\n",
    "            'Random': k = random between 2 and sqrt (sample n).\n",
    "\n",
    "            cons_validation: {'ac', 'anc', 'andc'}, default='ac'\n",
    "                Type of method to use to select the final partition.\n",
    "            'ac' : average Confidence of assignment of the objects to its clusters. \n",
    "            'anc': average Neighborhood Confidence of assigning the objects to its clusters.\n",
    "            'andc': average dynamic neighborhood confidence of assigning the objects to its clusters.\n",
    "\n",
    "            m : int, default=3\n",
    "                Number of neighbors, used when cons_validation = 'anc'.\n",
    "\n",
    "            alpha : float > 0, default=0.5\n",
    "                Use when cons_validation = 'etc' to dynamically calculate the number of neighbors.\n",
    "\n",
    "\n",
    "            Attributes\n",
    "            ----------\n",
    "            co_association_matrix : ndarray of shape (n_clusters, n_samples)\n",
    "                Co-association matrix.\n",
    "\n",
    "            labels_ : ndarray of shape (n_samples, )\n",
    "                Labels of each point, corresponds to the best partition selected by\n",
    "                one of the methods {'ac', 'anc', 'adnc'}\n",
    "\n",
    "            partitions : ndarray of shape(n_samples, 50)\n",
    "                Partitions generated by the ncut algorithm by removing negative evidence\n",
    "                from the co-association matrix.\n",
    "            \n",
    "            quality_of_partitions : ndarray of shape (50).\n",
    "                Quality measured on each partition \n",
    "                by one of the evaluation methods {'ac', 'anc', 'adnc'}\n",
    "        \"\"\"\n",
    "        self.__n_clusters = n_clusters\n",
    "        self.__n_partitions = 1000\n",
    "        self.__max_iter = max_iter\n",
    "        self.__k_type = k_type\n",
    "        self.__cons_validation = cons_validation\n",
    "        self.__m = m\n",
    "        self.__alpha = alpha\n",
    "        self.co_association_matrix = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "            Compute clusters.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            X : array, shape=(n_samples, n_features)\n",
    "            \n",
    "            Returns \n",
    "            -------\n",
    "            self\n",
    "                Fitted estimator.\n",
    "        \"\"\"\n",
    "        if type(X) == pd.DataFrame:\n",
    "            X = np.asarray(X)\n",
    "        \n",
    "        if len(self.co_association_matrix) == 0:\n",
    "            PI = gen_base_partition_by_kmeans(X, It=self.__max_iter, M=self.__n_partitions, Ktype=self.__k_type)\n",
    "            self.co_association_matrix = gen_cm(PI)\n",
    "        self.partitions = []\n",
    "        for i in np.arange(0.01, 0.51, 0.01):\n",
    "            CM_ = self.co_association_matrix.copy()\n",
    "            partition = ClusteringNormalizedCuts(self.__n_clusters).fit_predict(CM_)\n",
    "            self.partitions.append(partition)\n",
    "        self.partitions = np.transpose(self.partitions)\n",
    "        \n",
    "        self.quality_of_partitions = []\n",
    "        if self.__cons_validation == 'ac':\n",
    "            self.quality_of_partitions.append(np.apply_along_axis(lambda x: average_confidence(self.co_association_matrix,  x),\n",
    "                                     0, self.partitions))\n",
    "        elif self.__cons_validation == 'anc':\n",
    "            self.quality_of_partitions.append(np.apply_along_axis(lambda x: average_neighborhood_confidence(self.co_association_matrix, x,\n",
    "                                                                      self.__m),\n",
    "                                          0, self.partitions))\n",
    "        else:\n",
    "            self.quality_of_partitions.append(np.apply_along_axis(lambda x: average_dynamique_neighborhood_confidence(self.co_association_matrix,\n",
    "                                                                                              x, self.__alpha),\n",
    "                                          0, self.partitions))\n",
    "        self.quality_of_partitions = np.array(self.quality_of_partitions).ravel()\n",
    "        self.labels = self.partitions[:, self.quality_of_partitions.argmax()]\n",
    "        \n",
    "        \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "            Compute clusters  and predict clustered index for each sample.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            X : array, shape=(n_samples, n_features)\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            labels : array, shape [n_samples,]\n",
    "                Index of the cluster each sample belongs to.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels\n",
    "    \n",
    "    \n",
    "    def draw_vat(self, figsize=(10, 10)):\n",
    "        \"\"\"\n",
    "            Calculate and display the matrix for visual assessment of the cluster trend.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            figsize : tuple, default = (10, 10)\n",
    "                The dimensions of the figure.\n",
    "        \"\"\"\n",
    "        if len(self.co_association_matrix) == 0:\n",
    "            PI = gen_base_partition_by_kmeans(X, It=self.__max_iter, M=self.__n_partitions, Ktype=self.__k_type)\n",
    "            self.co_association_matrix = gen_cm(PI)        \n",
    "        vat_cm, index = vat(self.co_association_matrix)\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.title(\"VAT\")\n",
    "        im = plt.imshow(vat_cm, cmap='seismic')\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "_I9F5lliOxR0",
    "outputId": "fbc9de79-7575-41eb-c507-77a14286241b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "3959 documents\n",
      "4 categories\n",
      "\n",
      "Extracting features from the training dataset using a sparse vectorizer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "categories = [\n",
    "   'rec.autos', 'rec.sport.baseball','sci.crypt','sci.electronics',\n",
    "     \n",
    "]\n",
    "print(\"Extracting features from the training dataset \"\n",
    "      \"using a sparse vectorizer\")\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n",
    "\n",
    "labels_true = dataset.target\n",
    "true_k = np.unique(labels_true).shape[0]\n",
    "print(\"Extracting features from the training dataset \"\n",
    "      \"using a sparse vectorizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gZuh2hJooH85",
    "outputId": "8c3e7f01-a571-4d9c-d76b-4447f7e49027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "#fonction de lemmetisation et stematisation \n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# tokeniser et lemmatiser \n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "processed_docs = []\n",
    "\n",
    "for doc in dataset.data:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "news = []\n",
    "#retransformer le dataset en corpus de document \n",
    "for item in processed_docs:\n",
    "  t = ' '.join(item)\n",
    "  news.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRP-xMYFn26V"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,min_df=2)\n",
    "X = vectorizer.fit_transform(news)\n",
    "data = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wf0InZy6O3bD"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "svd_2 = TruncatedSVD(200)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd_2, normalizer)\n",
    "\n",
    "X_tranf = lsa.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCwRsNMCO7pA"
   },
   "outputs": [],
   "source": [
    "cluster_ensemble = CluterEnsemble( n_clusters=4).fit_predict(X_tranf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "WDhRbXK_O_Qa",
    "outputId": "400a2a0a-bdfd-4a86-a5c9-936dafabdc87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.522\n",
      "Completeness: 0.691\n",
      "V-measure: 0.595\n",
      "Adjusted Rand Index: 0.507\n",
      "Adjusted Mutual Information: 0.595\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, cluster_ensemble))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, cluster_ensemble))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, cluster_ensemble))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, cluster_ensemble))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, cluster_ensemble))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_ensemble_count.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
